import torch

import tqdm



class TextCLIP(torch.nn.Module):
    def __init__(self, model):
        super(TextCLIP, self).__init__()
        self.model = model

    def forward(self, text):
        return self.model.encode_text(text)

class ImageCLIP(torch.nn.Module):
    def __init__(self, model):
        super(ImageCLIP, self).__init__()
        self.model = model

    def forward(self, image):
        return self.model.encode_image(image)


def encode_image(model, fusion_model, device):
    model.eval()
    fusion_model.eval()

    with torch.no_grad():
        for iii, (image, _) in enumerate(tqdm(val_loader)):
            num_segments = 8  # config.data.num_segments
            image = image.view((-1, num_segments, 3) + image.size()[-2:])
            b, t, c, h, w = image.size()
            image_input = image.to(device).view(-1, c, h, w)
            image_features = model.encode_image(image_input).view(b, t, -1)
            image_features = fusion_model(image_features)
            image_features /= image_features.norm(dim=-1, keepdim=True)
